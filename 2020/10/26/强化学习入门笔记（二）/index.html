<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>强化学习入门笔记（二） | CM233's Blog</title><script src="https://cdn.bootcss.com/valine/1.4.4/Valine.min.js"></script><link rel="stylesheet" href="/css/arknights.css"><link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/highlight.js/10.1.2/styles/atom-one-dark-reasonable.min.css"><style>@font-face {
 font-family: BenderLight;
 src: local('Bender'), url("/font/BenderLight.ttf");
}
@font-face {
 font-family: 'JetBrains Mono';
 src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}</style><meta name="generator" content="Hexo 5.2.0"></head><body><header><nav><a href="/">Home</a><a href="/archives/">Archives</a><a href="/Blogroll/">Blogroll</a></nav></header><main style="flex-direction: row-reverse"><article><div id="post-bg"><div id="post-title"><div id="post-info"><span>date:<time datetime="2020-10-26T03:12:20.000Z" id="date"> 2020-10-26</time></span><br><span>updated:<time datetime="2020-10-31T09:07:08.689Z" id="updated"> 2020-10-31</time></span></div><h1>强化学习入门笔记（二）</h1><hr></div><div id="post-content"><p>2020年10月26日 星期一 11时16分46秒 CST<br>学习内容：<br>周博磊强化学习纲要</p>
<hr>
<h2 id="第一讲"><a href="#第一讲" class="headerlink" title="第一讲"></a>第一讲</h2><p>英文部分基本都为视频中原文  </p>
<h3 id="Difference-between-RL-and-Supervised-Learning"><a href="#Difference-between-RL-and-Supervised-Learning" class="headerlink" title="Difference between RL and Supervised Learning"></a>Difference between RL and Supervised Learning</h3><ol>
<li>Sequential data as input(not i.i.d) (i.i.d：独立同分布)  </li>
<li>The learner is <strong>not</strong> told which actions to take, but instead must discover which actions yield the most reward by trying them</li>
<li><strong>Trial-and-error exploration</strong>(balance between exploration&amp;exploitation)  </li>
<li>There is no supervisor, only a reward signal, which is also <strong>delayed</strong>(延迟奖励)</li>
</ol>
<blockquote>
<p>独立同分布:在概率统计理论中，如果变量序列或者其他随机变量有相同的概率分布，并且互相独立，那么这些随机变量是独立同分布。<br>在西瓜书中解释是：输入空间X中的所有样本服从一个隐含未知的分布，训练数据所有样本都是独立地从这个分布上采样而得。  </p>
</blockquote>
<h3 id="RL特点："><a href="#RL特点：" class="headerlink" title="RL特点："></a>RL特点：</h3><p>试错，探索<br>延迟奖励<br>时间很重要，RL的数据是时间上关联的（sequential），不一定是iid的，而不是iid则意味着数据相关性较强，传统机器学习训练起来不稳定。<br>agent的行为会影响数据，如果模型差，则生成的数据也差，强化学习的问题其实就是agent和环境交互。学习极大化奖励的策略  </p>
<p><strong>The key elements of an RL agent: model, value, policy</strong>    </p>
<hr>
<h2 id="第二讲-马尔科夫决策过程"><a href="#第二讲-马尔科夫决策过程" class="headerlink" title="第二讲 马尔科夫决策过程"></a>第二讲 马尔科夫决策过程</h2><p>Markov Chain-&gt;Markov Reward Process(MRP)-&gt;Markov Decision Process(MDP)<br>policy iteration<br>value iteration  </p>
<h3 id="马尔科夫链"><a href="#马尔科夫链" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h3><p>参考笔记（一）</p>
<h3 id="MRP马尔科夫奖励过程"><a href="#MRP马尔科夫奖励过程" class="headerlink" title="MRP马尔科夫奖励过程"></a>MRP马尔科夫奖励过程</h3><p>MRP=马尔科夫链+奖励  </p>
<h4 id="几个概念："><a href="#几个概念：" class="headerlink" title="几个概念："></a>几个概念：</h4><p>Horizon: Number of maximum time steps in each episode（每一个episode的最大步数）<br>Return: Discounted sum of rewards from step t to horizon<br>state value function Vt(s) for MRP: Expected return from t in state s (Present value of future rewards)  </p>
  
<blockquote>
<p>关于式中γ，笔记（一）中有记述，可参照阅读    </p>
</blockquote>
<h4 id="Bellman-equation"><a href="#Bellman-equation" class="headerlink" title="Bellman equation"></a>Bellman equation</h4><img src="/2020/10/26/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/MRP_Bellman.jpg" class>  
<p>s’表示未来状态，P（s’|s）代表转移概率</p>
<img src="/2020/10/26/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/MRP_matrix.jpg" class>  

<p>对于大型的MRP的价值向量V有以下计算方法：<br>DP、Monte-Carlo evaluation、temporal-Difference learning</p>
<h3 id="MDP马尔科夫决策过程"><a href="#MDP马尔科夫决策过程" class="headerlink" title="MDP马尔科夫决策过程"></a>MDP马尔科夫决策过程</h3><p>本节要点：MDP概念、policy evaluation on MDP、policy iteration、value iteration</p>
<img src="/2020/10/26/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/MDP.jpg" class>  
<p>相比马尔科夫奖励过程，MDP多了action（agent决定），多了决策的问题。根据当前状态和在该状态下采取的行为，会有不同的奖励。  </p>
<blockquote>
<p>价值函数参考笔记（一）和report(1)，详细推导最好可以完全弄懂（目前未解决）  </p>
</blockquote>
<h4 id="policy-evaluation"><a href="#policy-evaluation" class="headerlink" title="policy evaluation"></a>policy evaluation</h4><p>Objective: Evaluate the value of state given a policy π: compute Vπ(s).<br>Output: the value function under policy Vπ<br>Solution: iteration on Bellman expectation backup<br>Algorithm: <strong>Synchronous backup</strong>(转换成DP的迭代，反复迭代直到收敛，收敛时就是当前t的价值函数)<br>Convergence: V1-&gt;V2-&gt;…-&gt;Vπ  </p>
<p>例子：<a target="_blank" rel="noopener" href="https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html">https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html</a></p>
<p>policy evaluation的形式是，给定一个MDP，和对应的策略，可以得到马尔可夫链上value function的过程。MDP可以有最佳的价值函数（最佳指的是找到一个决策π使得价值函数上每个值为最大，可能有多个最佳决策）<br>寻找最优策略可以通过对Q函数进行极大化来解决。(argmax Q*(s,a)) </p>
<h4 id="Decision-Making-in-MDP"><a href="#Decision-Making-in-MDP" class="headerlink" title="Decision Making in MDP"></a>Decision Making in MDP</h4><img src="/2020/10/26/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/MDP_DecisionMaking.jpg" class>   
<p>DP是解MDP的predict和control问题的有效方式<br>寻找最优策略的具体方法：policy iteration &amp; value iteration(都是DP)</p>
<h4 id="policy-iteration"><a href="#policy-iteration" class="headerlink" title="policy iteration"></a>policy iteration</h4><p>step:Evaluate policy π （计算当前π的价值函数v,然后推算q） -&gt; Improve the policy(对q函数做贪心，即argmax)<br>反复以上过程，迭代至收敛。</p>
<h4 id="value-iteration"><a href="#value-iteration" class="headerlink" title="value iteration"></a>value iteration</h4><p>（待补充）</p>
<div id="paginator"></div></div><div id="post-footer"><hr><a href="/2020/10/20/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/">强化学习入门笔记（一） Next →</a><hr></div><div id="bottom-btn"><a id="to-index" href="#post-index" title="index">≡</a><a id="to-top" href="#post-title" title="to top">∧</a></div><div id="Valine"></div><script>new Valine({
 el: '#Valine'
 , appId: ''
 , appKey: ''
 , placeholder: '此条评论委托企鹅物流发送'
})</script></div></article><aside><div id="about"><a href="/" id="logo"><img src="https://ak.hypergryph.com/assets/index/images/ak/pc/faction/1.png" alt="Logo"></a><h1 id="Dr"><a href="/">CM233's Blog</a></h1><section id="total"><a id="total-archives" href="/archives"><span class="total-title">Archives Total:</span><span class="total-number">7</span></a><div id="total-tags"><span class="total-title">Tags:</span><span class="total-number">3</span></div><div id="total-categories"><span class="total-title">Categories:</span><span class="total-number">2</span></div></section></div><div id="aside-block" style="order: -1"><h1>INDEX</h1><div id="post-index"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E8%AE%B2"><span class="toc-number">1.</span> <span class="toc-text">第一讲</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Difference-between-RL-and-Supervised-Learning"><span class="toc-number">1.1.</span> <span class="toc-text">Difference between RL and Supervised Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RL%E7%89%B9%E7%82%B9%EF%BC%9A"><span class="toc-number">1.2.</span> <span class="toc-text">RL特点：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E8%AE%B2-%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B"><span class="toc-number">2.</span> <span class="toc-text">第二讲 马尔科夫决策过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE"><span class="toc-number">2.1.</span> <span class="toc-text">马尔科夫链</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MRP%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%A5%96%E5%8A%B1%E8%BF%87%E7%A8%8B"><span class="toc-number">2.2.</span> <span class="toc-text">MRP马尔科夫奖励过程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%87%A0%E4%B8%AA%E6%A6%82%E5%BF%B5%EF%BC%9A"><span class="toc-number">2.2.1.</span> <span class="toc-text">几个概念：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Bellman-equation"><span class="toc-number">2.2.2.</span> <span class="toc-text">Bellman equation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MDP%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B"><span class="toc-number">2.3.</span> <span class="toc-text">MDP马尔科夫决策过程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#policy-evaluation"><span class="toc-number">2.3.1.</span> <span class="toc-text">policy evaluation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Decision-Making-in-MDP"><span class="toc-number">2.3.2.</span> <span class="toc-text">Decision Making in MDP</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#policy-iteration"><span class="toc-number">2.3.3.</span> <span class="toc-text">policy iteration</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#value-iteration"><span class="toc-number">2.3.4.</span> <span class="toc-text">value iteration</span></a></li></ol></li></ol></li></ol></div></div><footer style="text-align: right"><nobr><span class="text-title">©</span><span class="text-content">2019 to 2020</span></nobr><wbr><nobr><span class="text-title">ICP</span><span class="text-content">--备案号--</span></nobr><wbr><wbr><nobr>published with&nbsp;<a target="_blank" rel="noopener" href="http://hexo.io">Hexo&nbsp;</a></nobr><wbr><nobr>Theme&nbsp;<a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknights&nbsp;</a></nobr><wbr><nobr>My Email : 515435960@qq.com&nbsp;</nobr></footer></aside></main><canvas id="canvas-dust"></canvas><script src="/js/arknights.js"></script><script src="https://cdn.bootcdn.net/ajax/libs/highlight.js/10.1.2/highlight.min.js"></script></body></html>